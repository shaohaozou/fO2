
#%%
import pandas as pd
import numpy as np

from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

from bioinfokit.visuz import cluster

import joblib

import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

#%%
def log_transfer(data):
    ### 1. La less than 0.1
    data = data.drop(data[data['La']>1].index)
    
    ### 2. P less than 2000
    data = data.drop(data[data['P']>2000].index)
    
    ### 3. Th/U between 0.2 to 4
    data['Th/U'] = data['Th'] / data['U']
    data = data.drop(data[data['Th/U'] < 0.1].index)
    data = data.drop(data[data['Th/U']>4].index)
    data = data.drop(['Th/U'], axis = 1)
    data = data.reset_index(drop = True)
    
    ele = ['P', 'Ti', 'Y', 'Nb', 'Hf', 'Th', 'U', 'La', 'Ce', 'Pr', 'Nd', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho','Er', 'Tm', 'Yb', 'Lu']
    
    df = data[ele]
    df[df <= 0] =np.nan
    
    ## Log transformation
    for i in range(len(ele)):
        df[ele[i]] = df[ele[i]].apply(np.log)
    
    df2 = pd.concat([data.drop(ele, axis =1),df], axis =1)
    return df2

#%%
def pca_process(data):
    ### 1. read the orignial data using for machine learning
    ### which is used for calculation the mean and the std log values (used for get the distribution)
    df1 = pd.read_csv("Appendix 2.csv", encoding='cp1252')
    df1 = log_transfer(df1)
    
    ### 1. log transfrom the calculated data
    data = log_transfer(data)
    ele = ['P', 'Ti', 'Y', 'Nb', 'Hf', 'Th', 'U', 'La', 'Ce', 'Pr', 'Nd', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho','Er', 'Tm', 'Yb', 'Lu']
    
    df =data[ele]
    #### filled the Nan values using normal distribution with mean and std.
    np.random.seed(1)
    for i in range(len(ele)):
        df[ele[i]][np.isnan(df[ele[i]])] = pd.DataFrame(np.random.normal(df1[ele[i]].mean(),df1[ele[i]].std(), len(df[ele[i]])))[0]
    
    pca_out = joblib.load('pca0928.pkl')
    pc_list = ["PC"+str(i) for i in list(range(1, 22))]
    pca_scores = pca_out.transform(df)
    indivaul_df = pd.DataFrame(pca_scores, columns=pc_list)
    
    ### chose sample number and PC1 to PC7
    df_ML = pd.concat([data.drop(ele, axis =1), indivaul_df.iloc[:, 0:7]], axis =1)
    
    return df_ML

#%%
#### Obtain the Machine learning train and test X, Y; and also the testset location
def get_ML_XY(data,i = 0):
    X = data.iloc[:, -7:]
    y = data['log fO2 (dFMQ)']
    
    df_ML = pd.concat([data.iloc[:, 0:1], X, data[['log fO2 (dFMQ)', 'log fO2 (dFMQ)_1std']]], axis=1)
    
    Train, test = train_test_split(df_ML, test_size = 0.2, random_state = i, stratify = df_ML.iloc[:, 0:1])
    
    X_train = Train.iloc[:, 1:8]
    y_train = Train['log fO2 (dFMQ)']
    
    X_test = test.iloc[:, 1:8]
    y_test = test['log fO2 (dFMQ)']
    test_loc = test[[test.columns.values[0], 'log fO2 (dFMQ)', 'log fO2 (dFMQ)_1std']]
    
    return X_train, y_train, X_test, y_test, test_loc

#%%
### Calculate the fo2 using the Ce, Ti, and age-corrected initial U concentrations in zircon
def louck_fo2(Age, U, Ti, Ce):
    Ui = U * (np.exp(Age*0.000000000155125*1000000)+0.0072*np.exp(Age*0.00000000098485*1000000))
    
    ## a is (Ce/Ui)*(Ui/Ti)^0.5
    a = (Ce/Ui)*((Ui/Ti)**0.5)
    
    dFMQ = 3.998*(np.log10(a)) +2.284
    
    return dFMQ

#%%